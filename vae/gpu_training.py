# -*- coding: utf-8 -*-
"""vae.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1mb6qkJGA_6KAWfP1kSJhLPMHVAKD9lNH
"""

# pip install torch matplotlib datasets torchvision tqdm

"""# Utils"""

import re
from datetime import datetime
from pathlib import Path

import torch.nn as nn

CHECKPOINT_DIR = Path("checkpoints")
SAMPLES_DIR = Path("samples")
FILENAME_PREFIX = "vae"


def _fmt_hms(seconds: float) -> str:
    s = int(seconds)
    m, s = divmod(s, 60)
    h, m = divmod(m, 60)
    return f"{h:02d}:{m:02d}:{s:02d}"


def _next_run_index(checkpoint_dir: Path = CHECKPOINT_DIR, prefix: str = FILENAME_PREFIX) -> int:
    """
    Scan existing checkpoints and return the next run index (1-based).
    Looks for files like: {prefix}_E###_I###_DYYYYMMDD-HHMMSS.pt
    """
    pattern = re.compile(rf"{re.escape(prefix)}_E\d+_I(\d+)_D\d{{8}}-\d{{6}}\.pt$")
    max_idx = 0
    for p in checkpoint_dir.glob(f"{prefix}_E*_I*_D*.pt"):
        m = pattern.match(p.name)
        if m:
            max_idx = max(max_idx, int(m.group(1)))
    return max_idx + 1


def _save_checkpoint(model: nn.Module, epoch: int, run_idx: int,
                     checkpoint_dir: Path = CHECKPOINT_DIR,
                     prefix: str = FILENAME_PREFIX) -> Path:
    datestr = datetime.now().strftime("%Y%m%d-%H%M%S")
    path = checkpoint_dir / f"{prefix}_E{epoch:03d}_I{run_idx:03d}_D{datestr}.pt"

    state_dict_fp16 = {k: (v.half() if torch.is_floating_point(v) else v) for (k,v) in model.state_dict().items()}
    torch.save(state_dict_fp16, path)

    print(f"\nSaved checkpoint: {path}")
    return path


def _find_latest_checkpoint(checkpoint_dir: Path = CHECKPOINT_DIR,
                            prefix: str = FILENAME_PREFIX) -> Path | None:
    """
    Find the latest checkpoint file, preferring parsed (epoch, index, date),
    but falling back to modification time when needed.
    """
    candidates = list(checkpoint_dir.glob(f"{prefix}_E*_I*_D*.pt"))
    if not candidates:
        return None

    rx = re.compile(rf"{re.escape(prefix)}_E(\d+)_I(\d+)_D(\d{{8}}-\d{{6}})\.pt$")

    def key(p: Path):
        m = rx.match(p.name)
        if m:
            e, i, ds = int(m.group(1)), int(m.group(2)), m.group(3)
            # Include st_mtime as final tie-breaker
            return (e, i, ds, p.stat().st_mtime)
        else:
            # Unparseable → rank lowest, sorted by modified time
            return (0, 0, "00000000-000000", p.stat().st_mtime)

    return max(candidates, key=key)

"""# Dataset class"""

from torch.utils.data import Dataset

class CelebAHFDataset(Dataset):
    def __init__(self, hf_dataset, transform=None):
        self.ds = hf_dataset
        self.transform = transform

    def __len__(self):
        return len(self.ds)

    def __getitem__(self, idx):
        img = self.ds[idx]["image"]
        if self.transform:
            img = self.transform(img)
        return img, 0  # dummy label

"""# Dataloaders"""

import time
from pathlib import Path

import torch
import torch.nn as nn
import torch.nn.functional as F
import torch.optim as optim
from torchvision import transforms, utils
from tqdm import tqdm
import matplotlib.pyplot as plt
from datasets import load_dataset
from torch.utils.data import DataLoader
import multiprocessing


device = "cpu"
dataset="flwrlabs/celeba"

if torch.cuda.is_available():
    device = torch.device("cuda")
else:
    device = torch.device("cpu")

print(f"Using device: {device}")

CHECKPOINT_DIR.mkdir(parents=True, exist_ok=True)
SAMPLES_DIR.mkdir(parents=True, exist_ok=True)

BATCH_SIZE = 1024
LR = 5e-4
SEED = 0
LATENT_DIM = 128
WARMUP_EPOCHS = 10
MAX_BETA = 2.0
NUM_WORKERS = min(32, multiprocessing.cpu_count() - 1)

torch.manual_seed(SEED)

transform = transforms.Compose([
        transforms.CenterCrop(178),
        transforms.Resize(128),
        transforms.ToTensor(),
        transforms.Normalize([0.5]*3, [0.5]*3) # Scale to [-1, 1]
    ])

print(f"\nLoading dataset from Hugging Face: {dataset}")
celeba = load_dataset(dataset)

full_train = celeba["train"]
split = full_train.train_test_split(test_size=0.1, seed=SEED)
train_dataset = CelebAHFDataset(split["train"], transform=transform)
val_dataset = CelebAHFDataset(split["test"], transform=transform)

train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True, num_workers=NUM_WORKERS,
                          pin_memory=True, prefetch_factor=4, persistent_workers=True)
val_loader = DataLoader(val_dataset, batch_size=BATCH_SIZE, shuffle=False, num_workers=max(1, NUM_WORKERS // 2),
                        pin_memory=True, persistent_workers=True, prefetch_factor=2)

# !nvidia-smi

"""# VAE class + loss"""

class VAE(nn.Module):
    def __init__(self, latent_dim=LATENT_DIM):
        super(VAE, self).__init__()

        self.enc_conv1 = nn.Conv2d(3, 64, kernel_size=4, stride=2, padding=1)
        self.enc_conv2 = nn.Conv2d(64, 128, kernel_size=4, stride=2, padding=1)
        self.enc_conv3 = nn.Conv2d(128, 256, kernel_size=4, stride=2, padding=1)

        self.fc_mu = nn.Linear(256 * 16 * 16, latent_dim)
        self.fc_logvar = nn.Linear(256 * 16 * 16, latent_dim)

        self.fc_decode = nn.Linear(latent_dim, 256 * 16 * 16)
        self.dec_conv1 = nn.ConvTranspose2d(256, 128, kernel_size=4, stride=2, padding=1)
        self.dec_conv2 = nn.ConvTranspose2d(128, 64, kernel_size=4, stride=2, padding=1)
        self.dec_conv3 = nn.ConvTranspose2d(64, 3, kernel_size=4, stride=2, padding=1)

    def encode(self, x):
        x = F.leaky_relu(self.enc_conv1(x))
        x = F.leaky_relu(self.enc_conv2(x))
        x = F.leaky_relu(self.enc_conv3(x))
        x = x.view(x.size(0), -1)
        mu = self.fc_mu(x)
        logvar = self.fc_logvar(x)
        return mu, logvar

    def reparameterize(self, mu, logvar):
        std = torch.exp(0.5 * logvar)
        eps = torch.randn_like(std)
        return mu + std * eps

    def decode(self, z):
        x = self.fc_decode(z)
        x = x.view(-1, 256, 16, 16)
        x = F.leaky_relu(self.dec_conv1(x))
        x = F.leaky_relu(self.dec_conv2(x))
        x = torch.tanh(self.dec_conv3(x))
        return x

    def forward(self, x):
        mu, logvar = self.encode(x)
        z = self.reparameterize(mu, logvar)
        return self.decode(z), mu, logvar


def vae_loss(recon_x, x, mu, logvar, epoch):
    with torch.amp.autocast("cuda", enabled=False):
        recon_loss = F.mse_loss(
            recon_x.float(), x.float(), reduction="sum"
        ) / x.size(0)
    kld = -0.5 * torch.sum(1 + logvar - mu.pow(2) - torch.exp(logvar)) / (x.size(0) * mu.size(1))

    beta = min(1.0, epoch / WARMUP_EPOCHS) * MAX_BETA
    return recon_loss + kld * beta, recon_loss, kld, beta

"""# Training loop"""

def train_vae(
    epochs=20,
    latent_dim=LATENT_DIM,
    short_run=False,
    checkpoint_dir: Path = CHECKPOINT_DIR,
    checkpoint_prefix: str = FILENAME_PREFIX,
):
    torch.backends.cudnn.benchmark = True
    model = VAE(latent_dim).to(device)
    optimizer = optim.Adam(model.parameters(), lr=LR)

    run_idx = _next_run_index(checkpoint_dir=checkpoint_dir, prefix=checkpoint_prefix)

    print("\nStarting training…")
    t0_total = time.perf_counter()

    for epoch in range(epochs if not short_run else 1):
        t0_epoch = time.perf_counter()

        model.train()
        total_recon, total_kld = 0.0, 0.0

        scaler = torch.amp.GradScaler("cuda")

        for batch_idx, (data, _) in enumerate(tqdm(train_loader)):
            data = data.to(device, non_blocking=True)

            optimizer.zero_grad()
            with torch.amp.autocast("cuda"):
                recon, mu, logvar = model(data)
                loss, recon_loss, kld, beta = vae_loss(recon, data, mu, logvar, epoch)

            scaler.scale(loss).backward()
            scaler.step(optimizer)
            scaler.update()

            total_recon += recon_loss.item()
            total_kld += kld.item()

            if short_run and batch_idx > 10:
                break

        beta = min(1.0, epoch / WARMUP_EPOCHS) * MAX_BETA
        avg_train_loss = (total_recon + beta * total_kld) / len(train_loader)
        avg_recon = total_recon / len(train_loader)
        avg_kld = total_kld / len(train_loader)

        model.eval()
        val_loss = 0.0
        with torch.no_grad():
            for val_data, _ in val_loader:
                val_data = val_data.to(device)
                recon, mu, logvar = model(val_data)
                current_val_loss, _, _, _ = vae_loss(recon, val_data, mu, logvar, epoch)
                val_loss += current_val_loss.item()
        avg_val_loss = val_loss / len(val_loader)

        elapsed_epoch = time.perf_counter() - t0_epoch

        print(f"Epoch {epoch:03d} | Train: {avg_train_loss:.4f} "
              f"(Recon: {avg_recon:.4f}, KL: {avg_kld:.4f}, β={beta:.2f}) "
              f"| Val: {avg_val_loss:.4f} | [TIME] {_fmt_hms(elapsed_epoch)}")

        if (epoch + 1) % 10 == 0 or epoch == epochs - 1:
            _save_checkpoint(
                model=model,
                epoch=epoch + 1,
                run_idx=run_idx,
                checkpoint_dir=checkpoint_dir,
                prefix=checkpoint_prefix
            )
            generate_faces_from_latest(
                latent_dim=latent_dim,
                num_samples=16,
                checkpoint_dir=checkpoint_dir,
                samples_dir=SAMPLES_DIR,
                prefix=checkpoint_prefix
            )

    elapsed_total = time.perf_counter() - t0_total
    print(f"\n[TIME] Full training took { _fmt_hms(elapsed_total) }")

    return model

def generate_faces_from_latest(
    latent_dim=LATENT_DIM,
    num_samples=16,
    checkpoint_dir: Path = CHECKPOINT_DIR,
    samples_dir: Path = SAMPLES_DIR,
    prefix: str = FILENAME_PREFIX
) -> Path | None:
    latest = _find_latest_checkpoint(checkpoint_dir=checkpoint_dir, prefix=prefix)
    if latest is None:
        print("\nNo checkpoints found.")
        return None

    model = VAE(latent_dim=latent_dim).to(device)
    state = torch.load(latest, map_location=device)
    model.load_state_dict(state)
    model.eval()

    with torch.no_grad():
        z = torch.randn(num_samples, latent_dim).to(device)
        samples = model.decode(z).cpu()
        grid = utils.make_grid(samples, nrow=4)
        plt.figure(figsize=(8, 8))
        plt.imshow(grid.permute(1, 2, 0))
        plt.axis("off")

        stem = latest.stem
        out_path = samples_dir / f"faces_{stem}_N{num_samples}.png"
        plt.savefig(out_path, bbox_inches="tight")
        plt.close()
        print(f"\nGenerated faces from {latest.name} -> {out_path}")
        return out_path


vae_model = train_vae(epochs=100, short_run=False)